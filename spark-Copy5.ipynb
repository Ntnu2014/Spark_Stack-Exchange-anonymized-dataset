{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark with Stack Exchange anonymized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark is used for data manipulation, analysis, and machine learning on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are three sub-folders, `allUsers`, `allPosts`, and `allVotes` \n",
    "#Gzipped XML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://mydata-course/spark-stats-data/stack_exchange_schema.txt ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can either get the data by running the appropriate S3 commands in the terminal, \n",
    "#or by running this block for the smaller stats data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p spark-stats-data\n",
    "!aws s3 sync --exclude '*' --include 'all*' s3://mydata-course/spark-stats-data/ ./spark-stats-data\n",
    "!aws s3 sync --exclude '*' --include 'posts*zip' s3://mydata-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p spark-stack-data\n",
    "!aws s3 sync --exclude '*' --include 'all*' s3://mydata-course/spark-stack-data/ ./spark-stack-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and parsing_Bad XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning the total number XML rows that started with <row\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "import os, time\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)\n",
    "\n",
    "def isEntry(line):\n",
    "    return \"<row \" in line\n",
    "\n",
    "import re\n",
    "def isValid(line):\n",
    "    patt_row = re.compile('<row.*/>')\n",
    "    return bool(re.search(patt_row, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entries = sc.textFile(localpath('spark-stats-data/allPosts/')) \\\n",
    "        .filter(lambda x: isEntry(x))\n",
    "\n",
    "valid_entries = sc.textFile(localpath('spark-stats-data/allPosts/')) \\\n",
    "        .filter(lambda x: isValid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upvote percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average percentage of upvotes (upvotes / (upvotes + downvotes)) \n",
    "#for the smallest 50 keys.\n",
    "\n",
    "from lxml import etree\n",
    "valid_entries = sc.textFile(localpath('spark-stats-data/allVotes/')) \\\n",
    "        .filter(lambda x: isValid(x))\n",
    "hm = valid_entries.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    def __init__(self, postId, favCount):\n",
    "        self.postId = postId\n",
    "        self.favCount = favCount\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        postId = etree.fromstring(line).attrib['Id']\n",
    "        if re.search('FavoriteCount', line):\n",
    "            favCount = int(etree.fromstring(line).attrib['FavoriteCount'])\n",
    "        else:\n",
    "            favCount = 0\n",
    "        return cls(postId, favCount)\n",
    "\n",
    "postData = sc.textFile(localpath(\"spark-stats-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(Post.parse)\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, postId, voteType):\n",
    "        self.postId = postId\n",
    "        self.voteType = voteType\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        postId = etree.fromstring(line).attrib['PostId']\n",
    "        voteType = etree.fromstring(line).attrib['VoteTypeId']\n",
    "        return cls(postId, voteType)\n",
    "\n",
    "voteData = sc.textFile(localpath(\"spark-stats-data/allVotes\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(Vote.parse)\n",
    "\n",
    "def tupref(upvote):\n",
    "    if upvote[1] == '2':\n",
    "        return (upvote[0],(1,1))\n",
    "    elif upvote[1] == '3':\n",
    "        return (upvote[0],(0,1))\n",
    "    else:\n",
    "        return (upvote[0],(0,0))\n",
    "def addtup(tup1,tup2):\n",
    "    return (tup1[0]+tup2[0], tup1[1]+tup2[1])\n",
    "\n",
    "a = postData.map(lambda p: (p.postId, p.favCount)) \\\n",
    "    .join(voteData.map(lambda v: (v.postId, v.voteType))) \\\n",
    "    .map(lambda x: (x[1][0], x[1][1])) \\\n",
    "    .map(tupref) \\\n",
    "    .reduceByKey(addtup) \\\n",
    "    .map(lambda x: (x[0], x[1][0]/x[1][1])) \\\n",
    "    .sortByKey() \\\n",
    "    .take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9762684266140068),\n",
       " (1, 0.978881830802238),\n",
       " (2, 0.9866769349249134),\n",
       " (3, 0.9898600949813888),\n",
       " (4, 0.9902785567395775)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the correlation between a user's reputation and the kind of posts they make.\n",
    "class PostQA:\n",
    "    def __init__(self, UserId, PostTypeId):\n",
    "        self.UserId = UserId\n",
    "        self.PostTypeId = PostTypeId\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        PostTypeId = etree.fromstring(line).attrib['PostTypeId']\n",
    "        if re.search('OwnerUserId', line):\n",
    "            UserId = etree.fromstring(line).attrib['OwnerUserId']\n",
    "        else:\n",
    "            UserId = '-9999'\n",
    "\n",
    "        return cls(UserId, PostTypeId)\n",
    "\n",
    "postQAData = sc.textFile(localpath(\"spark-stats-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(PostQA.parse)\n",
    "\n",
    "\n",
    "class User:\n",
    "    def __init__(self, UserId, Reputation):\n",
    "        self.UserId = UserId\n",
    "        self.Reputation = Reputation\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        UserId = etree.fromstring(line).attrib['Id']\n",
    "        Reputation = etree.fromstring(line).attrib['Reputation']\n",
    "        return cls(UserId, Reputation)\n",
    "\n",
    "userData = sc.textFile(localpath(\"spark-stats-data/allUsers\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(User.parse)\n",
    "\n",
    "def tupref(answer):\n",
    "    if answer[1][1] == '2':\n",
    "        return ((-int(answer[1][0]),answer[0]),(1,1))\n",
    "    elif answer[1][1] == '1':\n",
    "        return ((-int(answer[1][0]),answer[0]),(0,1))\n",
    "    else:\n",
    "        return ((-int(answer[1][0]),answer[0]),(0,0))\n",
    "    \n",
    "def addtup(tup1,tup2):\n",
    "    return (tup1[0]+tup2[0], tup1[1]+tup2[1])\n",
    "\n",
    "b = userData.map(lambda u: (u.UserId, u.Reputation)) \\\n",
    "    .join(postQAData.map(lambda p: (p.UserId, p.PostTypeId))) \\\n",
    "    .map(tupref) \\\n",
    "    .reduceByKey(addtup) \\\n",
    "    .sortByKey() \\\n",
    "    .map(lambda x: (int(x[0][1]), x[1][0]/x[1][1])) \\\n",
    "    .take(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for i in b:\n",
    "    s += i[1]\n",
    "b.append((-1, s/len(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  <row AnswerCount=\"1\" Body=\"&lt;p&gt;This is a homework problem that I figured having someone explain and solve would be helpful.&#10;Null hypothesis $H_0:\\\\mu =4$, $H_1:\\\\mu \\\\neq 4$. The sample is drawn from ${[x_1, x_2]}={[5,11]}$. Can $H_0$ be rejected at the 1% significance level? &lt;/p&gt;&#10;\" CommentCount=\"5\" CreationDate=\"2014-12-11T06:54:59.660\" Id=\"128622\" LastActivityDate=\"2014-12-11T18:46:51.217\" LastEditDate=\"2014-12-11T18:46:51.217\" LastEditorUserId=\"21121\" OwnerUserId=\"21121\" PostTypeId=\"1\" Score=\"0\" Tags=\"&lt;hypothesis-testing&gt;&lt;self-study&gt;\" Title=\"Suppose a random sample of size n=2 is generated from a $N(\\\\mu, \\\\sigma^2)$ population. Test the following hypothesis\" ViewCount=\"33\" />',\n",
       " '  <row AcceptedAnswerId=\"128643\" AnswerCount=\"1\" Body=\"&lt;p&gt;So I know that to find the coefficients of the BLP of some data is to use the formula,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\\\\vec{\\\\beta} = [{\\\\bf X}^{T}{\\\\bf X}]^{-1}{\\\\bf X}^{T}{\\\\bf Y}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I also want to find the variance, and I see this formula, classified as &quot;non-parametric&quot; and &quot;robust&quot; one:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\\\\hat{V}[\\\\vec{\\\\hat{\\\\beta}}] = [{\\\\bf X}^{T}{\\\\bf X}]^{-1}{\\\\bf X}^{T}\\\\text{diag}[({\\\\bf Y}-{\\\\bf X}\\\\vec{\\\\beta})^{2}]{\\\\bf X}[{\\\\bf X}^{T}{\\\\bf X}]^{-1}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the equation that I have comes from some notes that don\\'t explain all of what is meant here.  First of all, I don\\'t understand what is meant by $({\\\\bf Y}-{\\\\bf X}\\\\vec{\\\\beta})^{2}$ because the stuff inside the power is a vector--how do you square a vector?  Coordinate-wise?&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, I\\'m not sure what is meant by &quot;diag&quot;.  Is that the diagonalization of the matrix, i.e. the diagonal matrix of its eigenvalues?  I\\'m computing this in R and R has a canned command for selecting the diagonal elements from a matrix--is that what this is supposed to be?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have another confusion about this.  If you use this for a BLP with two variable terms and collect 10 data points, for instance, then $\\\\bf X$ will have dimensions 10x3 and since ${\\\\bf X}^{T}$ is to the left of the diag(...) factor, this must have 3 rows.  Since $\\\\bf X$ occurs to its right, then the diag(...) factor must have 3 columns.  But in that case, the result of this computation will be a $3 \\\\times 3$ matrix.  Since it\\'s supposed to tell me the variance of the coefficients, I don\\'t see how I would interpret this result.&lt;/p&gt;&#10;\" CommentCount=\"2\" CreationDate=\"2014-12-11T07:32:30.063\" Id=\"128624\" LastActivityDate=\"2014-12-11T10:25:28.430\" LastEditDate=\"2014-12-11T10:00:21.710\" LastEditorUserId=\"28666\" OwnerUserId=\"59093\" PostTypeId=\"1\" Score=\"3\" Tags=\"&lt;regression&gt;&lt;multiple-regression&gt;&lt;linear-algebra&gt;&lt;robust-standard-error&gt;\" Title=\"How to compute robust standard errors of the coefficients in multiple regression?\" ViewCount=\"39\" />']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(localpath('spark-stats-data/allPosts/')) \\\n",
    "        .filter(lambda x: isValid(x)) \\\n",
    "        .take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  <row AccountId=\"5872878\" CreationDate=\"2015-03-02T18:42:20.510\" DisplayName=\"Lars Reeker\" DownVotes=\"0\" Id=\"70185\" LastAccessDate=\"2015-03-02T18:42:20.510\" ProfileImageUrl=\"https://lh3.googleusercontent.com/-Y7GNsydm-mc/AAAAAAAAAAI/AAAAAAAADq8/15o5t99O5IU/photo.jpg\" Reputation=\"1\" UpVotes=\"0\" Views=\"0\" />',\n",
       " '  <row AccountId=\"5872995\" CreationDate=\"2015-03-02T19:04:13.380\" DisplayName=\"Vra\" DownVotes=\"0\" Id=\"70186\" LastAccessDate=\"2015-03-06T15:45:57.590\" Reputation=\"6\" UpVotes=\"0\" Views=\"1\" />']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(localpath('spark-stats-data/allUsers/')) \\\n",
    "        .filter(lambda x: isValid(x)) \\\n",
    "        .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning the top 100 post counts among all users (of all types of posts) \n",
    "#and the average reputation for every user who has that count.\n",
    "\n",
    "class PostCount:\n",
    "    def __init__(self, UserId, Id):\n",
    "        self.UserId = UserId\n",
    "        self.Id = Id\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        Id = etree.fromstring(line).attrib['Id']\n",
    "        if re.search('OwnerUserId', line):\n",
    "            UserId = etree.fromstring(line).attrib['OwnerUserId']\n",
    "        else:\n",
    "            UserId = '-9999'\n",
    "        return cls(UserId, Id)\n",
    "\n",
    "postCountData = sc.textFile(localpath(\"spark-stats-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(PostCount.parse)\n",
    "\n",
    "class User:\n",
    "    def __init__(self, UserId, Reputation):\n",
    "        self.UserId = UserId\n",
    "        self.Reputation = Reputation\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        UserId = etree.fromstring(line).attrib['Id']\n",
    "        Reputation = etree.fromstring(line).attrib['Reputation']\n",
    "        return cls(UserId, Reputation)\n",
    "\n",
    "userData = sc.textFile(localpath(\"spark-stats-data/allUsers\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(User.parse)\n",
    "\n",
    "c = postCountData.map(lambda p: (p.UserId, p.Id)) \\\n",
    "    .join(userData.map(lambda u: (u.UserId, u.Reputation))) \\\n",
    "    .map(lambda x: ((x[0], x[1][1]),1)) \\\n",
    "    .reduceByKey(lambda x, y : x + y) \\\n",
    "    .map(lambda x: (x[1], int(x[0][1]))) \\\n",
    "    .aggregateByKey((0,0),lambda a,b: (a[0] + b,  a[1] + 1),\n",
    "                           lambda a,b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "    .mapValues(lambda v: v[0]/v[1]) \\\n",
    "    .sortByKey(ascending= False) \\\n",
    "    .take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a list, whose i'th element correspond to i'th hour\n",
    "\n",
    "class QuestionCount:\n",
    "    def __init__(self, aId, qId, PostTypeId, qCreationDate):\n",
    "        self.qId = qId\n",
    "        self.aId = aId\n",
    "        self.PostTypeId = PostTypeId\n",
    "        self.qCreationDate = qCreationDate\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        qId = etree.fromstring(line).attrib['Id']\n",
    "        PostTypeId = etree.fromstring(line).attrib['PostTypeId']\n",
    "        qCreationDate = etree.fromstring(line).attrib['CreationDate']\n",
    "        if re.search('AcceptedAnswerId', line):\n",
    "            aId = etree.fromstring(line).attrib['AcceptedAnswerId']\n",
    "        else:\n",
    "            aId = '-999'\n",
    "        return cls(aId, qId, PostTypeId, qCreationDate)\n",
    "    \n",
    "QuestionData = sc.textFile(localpath(\"spark-stats-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(QuestionCount.parse)\n",
    "\n",
    "class AnsweredCount:\n",
    "    def __init__(self, aId, aType, aCreationDate):\n",
    "        self.aId = aId\n",
    "        self.aType = aType\n",
    "        self.aCreationDate = aCreationDate\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        aId = etree.fromstring(line).attrib['Id']\n",
    "        aType = etree.fromstring(line).attrib['PostTypeId']\n",
    "        aCreationDate = etree.fromstring(line).attrib['CreationDate']      \n",
    "        return cls(aId, aType, aCreationDate)\n",
    "    \n",
    "AnweredData = sc.textFile(localpath(\"spark-stats-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(AnsweredCount.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def onlyQA(x):\n",
    "    if x[1][1]=='1' and x[1][3]=='2':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def quickie(x):\n",
    "    qT = datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    aT = datetime.strptime(x[1][1], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    h_gap = (aT-qT).total_seconds()/(60*60)\n",
    "    return h_gap < 3\n",
    "\n",
    "\n",
    "quick_by_hour = QuestionData.map(lambda q: (q.aId, (q.qId, q.PostTypeId, q.qCreationDate))) \\\n",
    "    .join(AnweredData.map(lambda a: (a.aId, (a.aType, a.aCreationDate)))) \\\n",
    "    .map(lambda x: (x[0], x[1][0] + x[1][1])) \\\n",
    "    .filter(lambda x: onlyQA(x)) \\\n",
    "    .map(lambda x: (x[0],(x[1][2],x[1][4]))) \\\n",
    "    .filter(quickie) \\\n",
    "    .map(lambda x: (datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f').hour, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .collect()\n",
    "\n",
    "totAnswered_by_hour = QuestionData.map(lambda q: (q.aId, (q.qId, q.PostTypeId, q.qCreationDate))) \\\n",
    "    .join(AnweredData.map(lambda a: (a.aId, (a.aType, a.aCreationDate)))) \\\n",
    "    .map(lambda x: (x[0], x[1][0] + x[1][1])) \\\n",
    "    .filter(lambda x: onlyQA(x)) \\\n",
    "    .map(lambda x: (x[0],(x[1][2],x[1][4]))) \\\n",
    "    .map(lambda x: (datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f').hour, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_short = [i[1] for i in sorted(quick_by_hour, key = lambda x: x[0])]\n",
    "sorted_tot = [i[1] for i in sorted(totAnswered_by_hour, key = lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import truediv\n",
    "d = [truediv(*x) for x in zip(sorted_short, sorted_tot)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick answers&mdash;full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning a list, whose i'th element correspond to i'th hour \n",
    "#on the full Stack Exchange data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "line = '  <row Body=\"See `continuous-data`\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73934\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"4\" Score=\"0\" />'\n",
    "parsedline = etree.fromstring(line)\n",
    "'PostTypeId' in parsedline.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionCount:\n",
    "    def __init__(self, aId, qId, PostTypeId, qCreationDate):\n",
    "        self.qId = qId\n",
    "        self.aId = aId\n",
    "        self.PostTypeId = PostTypeId\n",
    "        self.qCreationDate = qCreationDate\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        qId = parsedline.attrib['Id']\n",
    "        PostTypeId = parsedline.attrib['PostTypeId']\n",
    "        qCreationDate = parsedline.attrib['CreationDate']\n",
    "        if 'AcceptedAnswerId' in parsedline.attrib:\n",
    "            aId = parsedline.attrib['AcceptedAnswerId']\n",
    "        else:\n",
    "            aId = '-999'\n",
    "        return cls(aId, qId, PostTypeId, qCreationDate)\n",
    "    \n",
    "QuestionData = sc.textFile(localpath(\"spark-stack-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(QuestionCount.parse)\n",
    "\n",
    "class AnsweredCount:\n",
    "    def __init__(self, aId, aType, aCreationDate):\n",
    "        self.aId = aId\n",
    "        self.aType = aType\n",
    "        self.aCreationDate = aCreationDate\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        aId = parsedline.attrib['Id']\n",
    "        aType = parsedline.attrib['PostTypeId']\n",
    "        aCreationDate = parsedline.attrib['CreationDate']      \n",
    "        return cls(aId, aType, aCreationDate)\n",
    "    \n",
    "AnweredData = sc.textFile(localpath(\"spark-stack-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(AnsweredCount.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyQA(x):\n",
    "    if x[1][1]=='1' and x[1][3]=='2':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def quickie(x):\n",
    "    qT = datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    aT = datetime.strptime(x[1][1], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    h_gap = (aT-qT).total_seconds()/(60*60)\n",
    "    return h_gap < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_by_hour = QuestionData.map(lambda q: (q.aId, (q.qId, q.PostTypeId, q.qCreationDate))) \\\n",
    "    .join(AnweredData.map(lambda a: (a.aId, (a.aType, a.aCreationDate)))) \\\n",
    "    .map(lambda x: (x[0], x[1][0] + x[1][1])) \\\n",
    "    .filter(lambda x: onlyQA(x)) \\\n",
    "    .map(lambda x: (x[0],(x[1][2],x[1][4]))) \\\n",
    "    .filter(quickie) \\\n",
    "    .map(lambda x: (datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f').hour, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .collect()\n",
    "\n",
    "totAnswered_by_hour = QuestionData.map(lambda q: (q.aId, (q.qId, q.PostTypeId, q.qCreationDate))) \\\n",
    "    .join(AnweredData.map(lambda a: (a.aId, (a.aType, a.aCreationDate)))) \\\n",
    "    .map(lambda x: (x[0], x[1][0] + x[1][1])) \\\n",
    "    .filter(lambda x: onlyQA(x)) \\\n",
    "    .map(lambda x: (x[0],(x[1][2],x[1][4]))) \\\n",
    "    .map(lambda x: (datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f').hour, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_short = [i[1] for i in sorted(quick_by_hour, key = lambda x: x[0])]\n",
    "sorted_tot = [i[1] for i in sorted(totAnswered_by_hour, key = lambda x: x[0])]\n",
    "e = [truediv(*x) for x in zip(sorted_short, sorted_tot)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify veterans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying \"veteran_score\",\"veteran_views\",\"vet_favorites\",\n",
    "# \"vet_answers\",\"brief_score\",\"brief_views\",\"brief_answers\",\"brief_favorites\"\n",
    "\n",
    "\n",
    "class postCount:\n",
    "    def __init__(self, UserId, pCreationDate, PostTypeId, Score, ViewCount, AnswerCount, FavoriteCount):\n",
    "        self.UserId = UserId\n",
    "        self.pCreationDate = pCreationDate\n",
    "        self.PostTypeId = PostTypeId\n",
    "        self.Score = Score\n",
    "        self.ViewCount = ViewCount\n",
    "        self.AnswerCount = AnswerCount\n",
    "        self.FavoriteCount = FavoriteCount\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        pCreationDate = parsedline.attrib['CreationDate']\n",
    "        PostTypeId = parsedline.attrib['PostTypeId']\n",
    "        \n",
    "        UserId = parsedline.attrib.get('OwnerUserId','-999')\n",
    "        Score = float(parsedline.attrib.get('Score',0))\n",
    "        ViewCount = float(parsedline.attrib.get('ViewCount',0))\n",
    "        AnswerCount = float(parsedline.attrib.get('AnswerCount',0))\n",
    "        FavoriteCount = float(parsedline.attrib.get('FavoriteCount',0))\n",
    "            \n",
    "        return cls(UserId, pCreationDate, PostTypeId, Score, ViewCount, AnswerCount, FavoriteCount)\n",
    "\n",
    "postCountData = sc.textFile(localpath(\"spark-stats-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(postCount.parse)\n",
    "\n",
    "class User:\n",
    "    def __init__(self, UserId, uCreationDate):\n",
    "        self.UserId = UserId\n",
    "        self.uCreationDate = uCreationDate\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        UserId = parsedline.attrib['Id']\n",
    "        uCreationDate = parsedline.attrib['CreationDate']\n",
    "        return cls(UserId, uCreationDate)\n",
    "\n",
    "userData = sc.textFile(localpath(\"spark-stats-data/allUsers\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(User.parse)\n",
    "\n",
    "\n",
    "def vetPotential(x):\n",
    "    uT = datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    pT = datetime.strptime(x[1][1], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    d_gap = (pT-uT).days\n",
    "    return int(d_gap < 150 and d_gap >= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isQuest(x):\n",
    "    return x[1][0]=='1'\n",
    "\n",
    "vedIds = userData.map(lambda u: (u.UserId, u.uCreationDate)) \\\n",
    "    .join(postCountData.map(lambda p: (p.UserId, p.pCreationDate))) \\\n",
    "    .map(lambda x: (x[0], vetPotential(x))) \\\n",
    "    .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24820"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vedIds.filter(lambda x: x[1] == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quest_1(x, y):\n",
    "    if datetime.strptime(x[1], '%Y-%m-%dT%H:%M:%S.%f') < datetime.strptime(y[1], '%Y-%m-%dT%H:%M:%S.%f'):\n",
    "        return x\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetTuple = vedIds.filter(lambda x: x[1] > 0) \\\n",
    "    .join(postCountData.map(lambda p: (p.UserId, (p.PostTypeId, p.pCreationDate, p.Score, p.ViewCount, p.AnswerCount, p.FavoriteCount)))) \\\n",
    "    .map(lambda x: (x[0], x[1][1])) \\\n",
    "    .filter(lambda x: isQuest(x)) \\\n",
    "    .reduceByKey(Quest_1) \\\n",
    "    .map(lambda x : (float(x[1][2]), float(x[1][3]), float(x[1][4]), float(x[1][5]), 1)) \\\n",
    "    .reduce(lambda x ,y : (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5434543454345433, 926.3982398239824, 1.2981298129812981, 1.300880088008801]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetlist = list(vetTuple)\n",
    "[i/vetlist[-1] for i in vetlist[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1008600836584104, 553.5200921182497, 0.9707195563284298, 0.5758800582788927]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefTuple = vedIds.filter(lambda x: x[1] == 0) \\\n",
    "    .join(postCountData.map(lambda p: (p.UserId, (p.PostTypeId, p.pCreationDate, p.Score, p.ViewCount, p.AnswerCount, p.FavoriteCount)))) \\\n",
    "    .map(lambda x: (x[0], x[1][1])) \\\n",
    "    .filter(lambda x: isQuest(x)) \\\n",
    "    .reduceByKey(Quest_1) \\\n",
    "    .map(lambda x : (float(x[1][2]), float(x[1][3]), float(x[1][4]), float(x[1][5]), 1)) \\\n",
    "    .reduce(lambda x ,y : (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4]))\n",
    "brieflist = list(briefTuple)\n",
    "[i/brieflist[-1] for i in brieflist[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify veterans&mdash;full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying veterans on full Stack Exchange data set\n",
    "\n",
    "class postCount:\n",
    "    def __init__(self, UserId, pCreationDate, PostTypeId, Score, ViewCount, AnswerCount, FavoriteCount):\n",
    "        self.UserId = UserId\n",
    "        self.pCreationDate = pCreationDate\n",
    "        self.PostTypeId = PostTypeId\n",
    "        self.Score = Score\n",
    "        self.ViewCount = ViewCount\n",
    "        self.AnswerCount = AnswerCount\n",
    "        self.FavoriteCount = FavoriteCount\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        pCreationDate = parsedline.attrib['CreationDate']\n",
    "        PostTypeId = parsedline.attrib['PostTypeId']\n",
    "        \n",
    "        UserId = parsedline.attrib.get('OwnerUserId','-999')\n",
    "        Score = float(parsedline.attrib.get('Score',0))\n",
    "        ViewCount = float(parsedline.attrib.get('ViewCount',0))\n",
    "        AnswerCount = float(parsedline.attrib.get('AnswerCount',0))\n",
    "        FavoriteCount = float(parsedline.attrib.get('FavoriteCount',0))\n",
    "            \n",
    "        return cls(UserId, pCreationDate, PostTypeId, Score, ViewCount, AnswerCount, FavoriteCount)\n",
    "\n",
    "postCountData = sc.textFile(localpath(\"spark-stack-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(postCount.parse)\n",
    "\n",
    "class User:\n",
    "    def __init__(self, UserId, uCreationDate):\n",
    "        self.UserId = UserId\n",
    "        self.uCreationDate = uCreationDate\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        UserId = parsedline.attrib['Id']\n",
    "        uCreationDate = parsedline.attrib['CreationDate']\n",
    "        return cls(UserId, uCreationDate)\n",
    "\n",
    "userData = sc.textFile(localpath(\"spark-stack-data/allUsers\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(User.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetPotential(x):\n",
    "    uT = datetime.strptime(x[1][0], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    pT = datetime.strptime(x[1][1], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    d_gap = (pT-uT).days\n",
    "    return int(d_gap < 150 and d_gap >= 100)\n",
    "\n",
    "def isQuest(x):\n",
    "    return x[1][0]=='1'\n",
    "\n",
    "def Quest_1(x, y):\n",
    "    if datetime.strptime(x[1], '%Y-%m-%dT%H:%M:%S.%f') < datetime.strptime(y[1], '%Y-%m-%dT%H:%M:%S.%f'):\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "vedIds = userData.map(lambda u: (u.UserId, u.uCreationDate)) \\\n",
    "    .join(postCountData.map(lambda p: (p.UserId, p.pCreationDate))) \\\n",
    "    .map(lambda x: (x[0], vetPotential(x))) \\\n",
    "    .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2598437331442924,\n",
       " 1844.0344896669696,\n",
       " 1.8426197044183144,\n",
       " 0.8673157237744455]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetTuple = vedIds.filter(lambda x: x[1] > 0) \\\n",
    "    .join(postCountData.map(lambda p: (p.UserId, (p.PostTypeId, p.pCreationDate, p.Score, p.ViewCount, p.AnswerCount, p.FavoriteCount)))) \\\n",
    "    .map(lambda x: (x[0], x[1][1])) \\\n",
    "    .filter(lambda x: isQuest(x)) \\\n",
    "    .reduceByKey(Quest_1) \\\n",
    "    .map(lambda x : (float(x[1][2]), float(x[1][3]), float(x[1][4]), float(x[1][5]), 1)) \\\n",
    "    .reduce(lambda x ,y : (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4]))\n",
    "\n",
    "vetlist = list(vetTuple)\n",
    "[i/vetlist[-1] for i in vetlist[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1307456144103445,\n",
       " 1096.1519220732553,\n",
       " 1.5038565525030159,\n",
       " 0.3861764445851408]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefTuple = vedIds.filter(lambda x: x[1] == 0) \\\n",
    "    .join(postCountData.map(lambda p: (p.UserId, (p.PostTypeId, p.pCreationDate, p.Score, p.ViewCount, p.AnswerCount, p.FavoriteCount)))) \\\n",
    "    .map(lambda x: (x[0], x[1][1])) \\\n",
    "    .filter(lambda x: isQuest(x)) \\\n",
    "    .reduceByKey(Quest_1) \\\n",
    "    .map(lambda x : (float(x[1][2]), float(x[1][3]), float(x[1][4]), float(x[1][5]), 1)) \\\n",
    "    .reduce(lambda x ,y : (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4]))\n",
    "brieflist = list(briefTuple)\n",
    "[i/brieflist[-1] for i in brieflist[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an alternative approach for vectorizing text data \n",
    "# for predicting other words in the document\n",
    "\n",
    "def isValid(line):\n",
    "    patt_row = re.compile('<row.*/>')\n",
    "    return bool(re.search(patt_row, line))\n",
    "\n",
    "def hasTag(x):\n",
    "    return x[1] != '-999'\n",
    "\n",
    "class postCount:\n",
    "    def __init__(self, Id, Tags):\n",
    "        self.Id = Id\n",
    "        self.Tags = Tags\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        Id = parsedline.attrib['Id']\n",
    "        \n",
    "        if 'Tags' in parsedline.attrib:\n",
    "            Tags = parsedline.attrib['Tags']\n",
    "        else:\n",
    "            Tags = '-999'            \n",
    "        return cls(Id, Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "posts = sc.textFile(localpath(\"spark-stack-data/allPosts\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(postCount.parse) \\\n",
    "    .map(lambda p: (p.Id, p.Tags)) \\\n",
    "    .filter(lambda x: hasTag(x)) \\\n",
    "    .map(lambda x: x[1]) \\\n",
    "    .map(lambda line: (re.findall(r'<(.*?)>',line),1)) \\\n",
    "    .toDF(['Tags', 'score'])\n",
    "\n",
    "\n",
    "w2v = Word2Vec(inputCol=\"Tags\", outputCol=\"vectors\", vectorSize=100, seed=42)\n",
    "model = w2v.fit(posts)\n",
    "result = model.transform(posts)\n",
    "\n",
    "vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "list_rows = model.findSynonyms('ggplot2', 25).rdd.map(lambda entry: (entry['word'], float(entry['similarity']))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Tags=['testing', 'ussd'], score=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the tags of a question from its body text using logistic regression\n",
    "\n",
    "class tagStuff:\n",
    "    def __init__(self, Tags, Body):\n",
    "        self.Tags = Tags\n",
    "        self.Body = Body\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        parsedline = etree.fromstring(line)\n",
    "        Tags = parsedline.attrib.get('Tags','-999')\n",
    "        Body = parsedline.attrib.get('Body','-999')            \n",
    "        return cls(Tags, Body)\n",
    "\n",
    "def hasTag(x):\n",
    "    return x[0] != '-999'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_count = sc.textFile(localpath(\"spark-stats-data/training\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(tagStuff.parse) \\\n",
    "    .map(lambda p: (p.Tags, p.Body)) \\\n",
    "    .filter(lambda x: hasTag(x)) \\\n",
    "    .map(lambda line: (re.findall(r'<(.*?)>',line[0]),line[1])) \\\n",
    "    .flatMap(lambda x: [(i, x[1]) for i in x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_count_dict = tag_count.countByKey()\n",
    "tag_100 = [tag[0] for tag in sorted(tag_count_dict.items(), key = lambda x: x[1], reverse = True)[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "test_tag_count = sc.textFile(localpath(\"spark-stats-data/test\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(tagStuff.parse) \\\n",
    "    .map(lambda p: (p.Tags, p.Body)) \\\n",
    "    .filter(lambda x: hasTag(x))\n",
    "\n",
    "test_data = sqlContext.createDataFrame(test_tag_count.map(lambda x: (x[1], x[0])), [\"Body\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4649"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(localpath(\"spark-stats-data/test\")) \\\n",
    "    .filter(lambda x: isValid(x)) \\\n",
    "    .map(tagStuff.parse) \\\n",
    "    .map(lambda p: (p.Tags, p.Body)) \\\n",
    "    .filter(lambda x: hasTag(x)) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Body\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "logreg = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_training_data = sqlContext.createDataFrame(tag_count.map(lambda x: (x[1], int(x[0] == 'r'))), [\"Body\", \"label\"])\n",
    "model = pipeline.fit(r_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r', 'regression', 'time-series', 'machine-learning', 'probability']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_100[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_probs = []\n",
    "\n",
    "for tags in tag_100:\n",
    "    training = sqlContext.createDataFrame(tag_count.map(lambda x: (x[1], int(x[0] == tags))), [\"Body\", \"label\"]).cache()\n",
    "    model = pipeline.fit(training)\n",
    "    predictions = model.transform(test_data).select(\"probability\").collect()\n",
    "    pred_list = [i[0][0] for i in predictions]\n",
    "    tag_probs.append((tags,pred_list))\n",
    "    training.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
